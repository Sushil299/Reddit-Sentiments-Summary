{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Qi-7J2wRnuP"
      },
      "source": [
        "# Market Sentiment Prediction Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZDezSu3K87q"
      },
      "outputs": [],
      "source": [
        "!pip install praw openai pandas asyncpraw textblob pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIp_gQwlBkzO"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLKlUEtzCQqJ"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load FinBERT tokenizer and model\n",
        "MODEL_NAME = \"yiyanghkust/finbert-tone\"\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = BertForSequenceClassification.from_pretrained(MODEL_NAME)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "IQE7GL9ehZ4I",
        "outputId": "1eab6755-b69d-443a-cd30-76e33fa1a4fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:asyncio:Unclosed client session\n",
            "client_session: <aiohttp.client.ClientSession object at 0x7cbcd85b3f10>\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Scraped data saved successfully!\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_f481fde7-d698-44d4-acfd-1fe31785749f\", \"reddit_stock_data_raw.json\", 804163)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import asyncpraw\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Configurable time period for fetching posts (in days)\n",
        "DAYS_BACK = 90  # Change this value to adjust the time range\n",
        "\n",
        "# Minimum engagement thresholds\n",
        "MIN_UPVOTES = 100\n",
        "MIN_COMMENTS = 10\n",
        "MIN_COMMENT_UPVOTES = 20\n",
        "MIN_COMMENT_LENGTH = 30\n",
        "\n",
        "# Initialize asyncpraw client\n",
        "reddit = asyncpraw.Reddit(\n",
        "    client_id=\"Enter_You_ID\",\n",
        "    client_secret=\"Enter_Your_Secret\",\n",
        "    user_agent=\"StockScraper\"\n",
        ")\n",
        "\n",
        "# Relevant Stock Market Subreddits\n",
        "subreddits = [\"IndianStockMarket\", \"DalalStreetTalks\", \"StockMarketIndia\", \"IndianStreetBets\", \"NSEBets\", \"ShareMarketupdates\"]\n",
        "\n",
        "# Keywords to filter out low-effort posts\n",
        "low_effort_keywords = [\"meme\", \"joke\", \"funny\", \"shitpost\", \"lol\", \"haha\", \"troll\"]\n",
        "\n",
        "# Define time threshold\n",
        "days_ago = datetime.utcnow() - timedelta(days=DAYS_BACK)\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces and newlines\n",
        "    return text\n",
        "\n",
        "# Fetch and process Reddit posts\n",
        "async def fetch_reddit_data():\n",
        "    data = []\n",
        "    for subreddit in subreddits:\n",
        "        subreddit_obj = await reddit.subreddit(subreddit)\n",
        "        async for post in subreddit_obj.top(time_filter='all', limit=500):  # Fetch more posts and filter manually\n",
        "\n",
        "            # Extract Post Details\n",
        "            post_id = post.id\n",
        "            title = clean_text(post.title)\n",
        "            body = clean_text(post.selftext)\n",
        "            upvotes = post.score\n",
        "            post_date_utc = datetime.utcfromtimestamp(post.created_utc)\n",
        "            post_date = post_date_utc.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "            # Skip posts older than the configured time period\n",
        "            if post_date_utc < days_ago:\n",
        "                continue\n",
        "\n",
        "            # Apply filters to remove low-quality posts\n",
        "            if any(word in title.lower() for word in low_effort_keywords):\n",
        "                continue\n",
        "            if len(title) < 30 and not body:  # Skip very short titles with no content\n",
        "                continue\n",
        "            if upvotes < MIN_UPVOTES:  # Skip low-engagement posts\n",
        "                continue\n",
        "\n",
        "            # Fetch & Store Top Comments\n",
        "            comments_list = []\n",
        "            try:\n",
        "                submission = await reddit.submission(id=post.id)\n",
        "                if submission.num_comments >= MIN_COMMENTS and not submission.locked:  # Ensure sufficient comments & post isn't locked\n",
        "                    await submission.comments.replace_more(limit=0)  # Fetch all comments\n",
        "                    all_comments = submission.comments.list()  # Get all comments\n",
        "\n",
        "                    # Filter valid comments\n",
        "                    valid_comments = [\n",
        "                        c for c in all_comments\n",
        "                        if hasattr(c, \"body\") and c.body not in [\"[deleted]\", \"[removed]\"]\n",
        "                        and c.score >= MIN_COMMENT_UPVOTES and len(c.body) >= MIN_COMMENT_LENGTH\n",
        "                    ]\n",
        "\n",
        "                    if valid_comments:\n",
        "                        comments_list = [{\n",
        "                            \"text\": clean_text(c.body),\n",
        "                            \"upvotes\": c.score\n",
        "                        } for c in valid_comments]\n",
        "\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            # Skip if there are no valid comments\n",
        "            if not comments_list:\n",
        "                continue\n",
        "\n",
        "            # Store Processed Data\n",
        "            data.append({\n",
        "                \"Post ID\": post_id,\n",
        "                \"Title\": title,\n",
        "                \"Post Content\": body,\n",
        "                \"Post Upvotes\": upvotes,\n",
        "                \"Comments\": comments_list,\n",
        "                \"Date\": post_date,\n",
        "                \"Source Subreddit\": subreddit\n",
        "            })\n",
        "\n",
        "    return data\n",
        "\n",
        "# Run the function and clean data\n",
        "async def main():\n",
        "    data = await fetch_reddit_data()\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Save scraped data to JSON format\n",
        "    df.to_json(\"reddit_stock_data_raw.json\", orient=\"records\", indent=4)\n",
        "    print(\"âœ… Scraped data saved successfully!\")\n",
        "\n",
        "# Execute async function\n",
        "await main()\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"reddit_stock_data_raw.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "DSTFighmT47-",
        "outputId": "9ebe9aa8-134e-46cf-aac6-c7df8ccb88bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Sentiment analysis completed and saved successfully!\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_76d80ee5-76d3-48da-b240-6ed849cab2cf\", \"reddit_stock_data_sentiments.json\", 1062841)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Part 2: Sentiment Analysis\n",
        "from transformers import AutoTokenizer, pipeline # Import AutoTokenizer\n",
        "import pandas as pd # Import pandas and alias it as 'pd'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "sentiment_analyzer = pipeline(\"text-classification\", model=\"ProsusAI/finbert\")\n",
        "\n",
        "# Function to get sentiment scores\n",
        "def analyze_sentiment(text):\n",
        "    tokens = tokenizer(text, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "    truncated_text = tokenizer.decode(tokens[\"input_ids\"][0], skip_special_tokens=True)\n",
        "    result = sentiment_analyzer(truncated_text, return_all_scores=True)[0]\n",
        "    sentiment_scores = {sent[\"label\"].lower(): sent[\"score\"] for sent in result}\n",
        "    overall_sentiment = max(sentiment_scores, key=sentiment_scores.get)\n",
        "    confidence = sentiment_scores[overall_sentiment]\n",
        "\n",
        "    return {\n",
        "        \"overall_sentiment\": overall_sentiment,\n",
        "        \"confidence\": confidence\n",
        "    }\n",
        "\n",
        "# Load scraped data\n",
        "df = pd.read_json(\"reddit_stock_data_raw.json\")\n",
        "\n",
        "# Apply sentiment analysis\n",
        "df[\"Post Sentiment\"] = df.apply(lambda row: analyze_sentiment(row[\"Title\"] + \" \" + row[\"Post Content\"]), axis=1)\n",
        "for index, row in df.iterrows():\n",
        "    for comment in row[\"Comments\"]:\n",
        "        comment.update(analyze_sentiment(comment[\"text\"]))\n",
        "\n",
        "# Save processed data\n",
        "df.to_json(\"reddit_stock_data_sentiments.json\", orient=\"records\", indent=4)\n",
        "print(\"âœ… Sentiment analysis completed and saved successfully!\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"reddit_stock_data_sentiments.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "SsrdP45qY1fz",
        "outputId": "3d966354-8dbd-4a90-a688-6a3de7ed3152"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Categorization completed and saved successfully!\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_7313676d-2102-4ab1-8046-f0561bbc8f82\", \"reddit_stock_data_category.json\", 1079179)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load Zero-Shot Classification Model\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Define possible categories\n",
        "CATEGORY_LABELS = [\n",
        "    \"Market News\",\n",
        "    \"Stock-Specific Discussion\",\n",
        "    \"Investment Idea\",\n",
        "    \"Trading Strategy\",\n",
        "    \"General Discussion\",\n",
        "    \"Regulatory News\",\n",
        "    \"Macroeconomy\",\n",
        "    \"Technical Analysis\",\n",
        "    \"Fundamental Analysis\"\n",
        "]\n",
        "\n",
        "# Function to classify posts\n",
        "def classify_post(title, content, comments):\n",
        "    # Combine post title, content, and top comments for classification\n",
        "    combined_text = title + \" \" + content + \" \" + \" \".join([c[\"text\"] for c in comments])\n",
        "\n",
        "    # Run zero-shot classification\n",
        "    result = classifier(combined_text, CATEGORY_LABELS, multi_label=False)\n",
        "\n",
        "    # Get the highest confidence category\n",
        "    category = result[\"labels\"][0]\n",
        "\n",
        "    return category\n",
        "\n",
        "# Load processed data with sentiment\n",
        "df = pd.read_json(\"reddit_stock_data_sentiments.json\")\n",
        "\n",
        "# Assign category to each post\n",
        "df[\"Category\"] = df.apply(lambda row: classify_post(row[\"Title\"], row[\"Post Content\"], row[\"Comments\"]), axis=1)\n",
        "\n",
        "# Save categorized data\n",
        "df.to_json(\"reddit_stock_data_category.json\", orient=\"records\", indent=4)\n",
        "print(\"âœ… Categorization completed and saved successfully!\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"reddit_stock_data_category.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2lNq503vcFoC",
        "outputId": "2212c29f-baa7-4a81-a9d8-1b176be57297"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Nifty 50 data merged with Reddit sentiment!\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_990563ab-2759-4733-b43a-b5b5308128df\", \"reddit_nifty_combined.json\", 813219)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Merging Nifty50 Data\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "# Fetch Nifty 50 historical data\n",
        "def fetch_nifty50_data(days=365):\n",
        "    nifty = yf.Ticker(\"^NSEI\")  # Nifty 50 index\n",
        "    nifty_data = nifty.history(period=f\"{days}d\")  # Get daily data for given period\n",
        "    nifty_data = nifty_data.reset_index()[[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
        "    nifty_data[\"Date\"] = nifty_data[\"Date\"].dt.strftime('%Y-%m-%d')  # Format date\n",
        "    return nifty_data\n",
        "\n",
        "# Load Reddit sentiment data\n",
        "reddit_data = pd.read_json(\"reddit_stock_data_category.json\")\n",
        "\n",
        "# Convert Reddit Date column to string (if it's not already)\n",
        "reddit_data[\"Date\"] = pd.to_datetime(reddit_data[\"Date\"]).dt.strftime('%Y-%m-%d')\n",
        "\n",
        "# Fetch market data\n",
        "nifty_data = fetch_nifty50_data(365)\n",
        "\n",
        "# Merge sentiment with market data based on date\n",
        "merged_data = pd.merge(reddit_data, nifty_data, on=\"Date\", how=\"inner\")\n",
        "\n",
        "# Save merged dataset\n",
        "merged_data.to_json(\"reddit_nifty_combined.json\", orient=\"records\", indent=4)\n",
        "\n",
        "print(\"âœ… Nifty 50 data merged with Reddit sentiment!\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"reddit_nifty_combined.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd2-HM_EiOSl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# Load the dataset (merged Reddit + Nifty data)\n",
        "data = pd.read_json(\"reddit_nifty_combined.json\")\n",
        "\n",
        "# Convert Date to datetime\n",
        "data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
        "\n",
        "# Calculate Nifty % return\n",
        "data[\"Nifty Return\"] = data[\"Close\"].pct_change() * 100\n",
        "\n",
        "# Drop first row (NaN due to pct_change)\n",
        "data = data.dropna()\n",
        "\n",
        "### ðŸŽ¯ Step 1: Compute Total Positive, Negative & Neutral Sentiments ðŸŽ¯ ###\n",
        "def calculate_sentiment_totals(row):\n",
        "    total_positive = sum(comment[\"upvotes\"] for comment in row[\"Comments\"] if comment[\"overall_sentiment\"] == \"positive\")\n",
        "    total_negative = sum(comment[\"upvotes\"] for comment in row[\"Comments\"] if comment[\"overall_sentiment\"] == \"negative\")\n",
        "    total_neutral = sum(comment[\"upvotes\"] for comment in row[\"Comments\"] if comment[\"overall_sentiment\"] == \"neutral\")\n",
        "\n",
        "    return pd.Series([total_positive, total_negative, total_neutral])\n",
        "\n",
        "# Apply sentiment calculations\n",
        "data[[\"Total Positive Sentiment\", \"Total Negative Sentiment\", \"Total Neutral Sentiment\"]] = data.apply(calculate_sentiment_totals, axis=1)\n",
        "\n",
        "# Aggregate daily sentiment scores\n",
        "daily_sentiment = data.groupby(\"Date\")[[\"Total Positive Sentiment\", \"Total Negative Sentiment\", \"Total Neutral Sentiment\"]].sum().reset_index()\n",
        "\n",
        "# Merge with Nifty data\n",
        "final_data = pd.merge(daily_sentiment, data[[\"Date\", \"Nifty Return\"]].drop_duplicates(), on=\"Date\", how=\"inner\")\n",
        "\n",
        "# Shift Nifty Return to T+1 (to make it the prediction target)\n",
        "final_data[\"Target Return\"] = final_data[\"Nifty Return\"].shift(-1)\n",
        "\n",
        "# Drop last row (since it has NaN target)\n",
        "final_data = final_data.dropna()\n",
        "\n",
        "# Save prepared dataset\n",
        "final_data.to_csv(\"nifty_sentiment_data.csv\", index=False)\n",
        "print(\"âœ… Data prepared for ML model!\")\n",
        "\n",
        "\n",
        "### ðŸ“Š Step 2: Train XGBoost Model ðŸ“Š ###\n",
        "# Load the processed dataset\n",
        "df = pd.read_csv(\"nifty_sentiment_data.csv\")\n",
        "\n",
        "# Features & target\n",
        "X = df[[\"Total Positive Sentiment\", \"Total Negative Sentiment\", \"Total Neutral Sentiment\"]]\n",
        "y = df[\"Target Return\"]\n",
        "\n",
        "# Split into train & test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train XGBoost model\n",
        "model = xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=100, learning_rate=0.1, max_depth=4)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"ðŸ“‰ Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "print(f\"ðŸ“ˆ RÂ² Score: {r2:.4f}\")\n",
        "print(\"âœ… XGBoost Model trained & saved!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx06NuRuSFZm"
      },
      "source": [
        "# Reddit Discussion Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9-FQ5GMCbphm"
      },
      "outputs": [],
      "source": [
        "''''\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "import json\n",
        "import textwrap\n",
        "\n",
        "# Load summarization model\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Load the scraped Reddit data\n",
        "with open(\"reddit_data_raw.json\", \"r\") as f:\n",
        "    reddit_data = json.load(f)\n",
        "\n",
        "# Combine all posts and comments into one large text block\n",
        "all_text = \" \".join(\n",
        "    post[\"Title\"] + \" \" + post[\"Post Content\"] + \" \" +\n",
        "    \" \".join(comment[\"text\"] for comment in post[\"Comments\"])\n",
        "    for post in reddit_data\n",
        ")\n",
        "\n",
        "# Break text into smaller chunks (each ~1000 characters to fit model limits)\n",
        "text_chunks = textwrap.wrap(all_text, width=1000)\n",
        "\n",
        "# Generate summaries for each chunk and combine\n",
        "summaries = [summarizer(chunk, max_length=200, min_length=50, do_sample=False)[0][\"summary_text\"] for chunk in text_chunks]\n",
        "\n",
        "# Final combined summary\n",
        "overall_summary = \" \".join(summaries)\n",
        "print(overall_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxIbiiW0SOAO"
      },
      "outputs": [],
      "source": [
        "!pip install praw pandas asyncpraw textblob pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RuaPHDXlSWvW"
      },
      "outputs": [],
      "source": [
        "import asyncpraw\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Configurable time period for fetching posts (in days)\n",
        "DAYS_BACK = 120  # Change this value to adjust the time range\n",
        "\n",
        "# Minimum engagement thresholds\n",
        "Num_Posts = 20 #Number of Posts to be scraped - Lower number since using paid version\n",
        "MIN_UPVOTES = 100\n",
        "MIN_COMMENTS = 10\n",
        "MIN_COMMENT_UPVOTES = 20\n",
        "MIN_COMMENT_LENGTH = 30\n",
        "\n",
        "# Initialize asyncpraw client\n",
        "reddit = asyncpraw.Reddit(\n",
        "    client_id=\"Enter_Your_Client_ID\",\n",
        "    client_secret=\"Enter_Your_Secret_Key\",\n",
        "    user_agent=\"StockScraper\"\n",
        ")\n",
        "\n",
        "# Relevant Stock Market Subreddits\n",
        "subreddits = [\"IndianStockMarket\", \"DalalStreetTalks\", \"StockMarketIndia\", \"IndianStreetBets\", \"NSEBets\", \"ShareMarketupdates\"]\n",
        "\n",
        "# Keywords to filter out low-effort posts\n",
        "low_effort_keywords = [\"meme\", \"joke\", \"funny\", \"shitpost\", \"lol\", \"haha\", \"troll\"]\n",
        "\n",
        "# Define time threshold\n",
        "days_ago = datetime.utcnow() - timedelta(days=DAYS_BACK)\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces and newlines\n",
        "    return text\n",
        "\n",
        "# Fetch and process Reddit posts\n",
        "async def fetch_reddit_data():\n",
        "    data = []\n",
        "    for subreddit in subreddits:\n",
        "        subreddit_obj = await reddit.subreddit(subreddit)\n",
        "        async for post in subreddit_obj.top(time_filter='all', limit= Num_Posts):  # Fetch more posts and filter manually\n",
        "\n",
        "            # Extract Post Details\n",
        "            post_id = post.id\n",
        "            title = clean_text(post.title)\n",
        "            body = clean_text(post.selftext)\n",
        "            upvotes = post.score\n",
        "            post_date_utc = datetime.utcfromtimestamp(post.created_utc)\n",
        "            post_date = post_date_utc.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "            # Skip posts older than the configured time period\n",
        "            if post_date_utc < days_ago:\n",
        "                continue\n",
        "\n",
        "            # Apply filters to remove low-quality posts\n",
        "            if any(word in title.lower() for word in low_effort_keywords):\n",
        "                continue\n",
        "            if len(title) < 30 and not body:  # Skip very short titles with no content\n",
        "                continue\n",
        "            if upvotes < MIN_UPVOTES:  # Skip low-engagement posts\n",
        "                continue\n",
        "\n",
        "            # Fetch & Store Top Comments\n",
        "            comments_list = []\n",
        "            try:\n",
        "                submission = await reddit.submission(id=post.id)\n",
        "                if submission.num_comments >= MIN_COMMENTS and not submission.locked:  # Ensure sufficient comments & post isn't locked\n",
        "                    await submission.comments.replace_more(limit=0)  # Fetch all comments\n",
        "                    all_comments = submission.comments.list()  # Get all comments\n",
        "\n",
        "                    # Filter valid comments\n",
        "                    valid_comments = [\n",
        "                        c for c in all_comments\n",
        "                        if hasattr(c, \"body\") and c.body not in [\"[deleted]\", \"[removed]\"]\n",
        "                        and c.score >= MIN_COMMENT_UPVOTES and len(c.body) >= MIN_COMMENT_LENGTH\n",
        "                    ]\n",
        "\n",
        "                    if valid_comments:\n",
        "                        comments_list = [{\n",
        "                            \"text\": clean_text(c.body),\n",
        "                            \"upvotes\": c.score\n",
        "                        } for c in valid_comments]\n",
        "\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            # Skip if there are no valid comments\n",
        "            if not comments_list:\n",
        "                continue\n",
        "\n",
        "            # Store Processed Data\n",
        "            data.append({\n",
        "                \"Post ID\": post_id,\n",
        "                \"Title\": title,\n",
        "                \"Post Content\": body,\n",
        "                \"Post Upvotes\": upvotes,\n",
        "                \"Comments\": comments_list,\n",
        "                \"Date\": post_date,\n",
        "                \"Source Subreddit\": subreddit\n",
        "            })\n",
        "\n",
        "    return data\n",
        "\n",
        "# Run the function and clean data\n",
        "async def main():\n",
        "    data = await fetch_reddit_data()\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Save scraped data to JSON format\n",
        "    df.to_json(\"reddit_data_raw.json\", orient=\"records\", indent=4)\n",
        "\n",
        "\n",
        "# Execute async function\n",
        "await main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Load the JSON file containing Reddit stock discussions\n",
        "with open(\"reddit_data_raw.json\", \"r\") as f:\n",
        "    reddit_data = json.load(f)\n",
        "\n",
        "# Extract relevant text from posts and comments\n",
        "all_text = \"\\n\".join(\n",
        "    post[\"Title\"] + \" \" + post[\"Post Content\"] + \" \" +\n",
        "    \" \".join(comment[\"text\"] for comment in post[\"Comments\"])\n",
        "    for post in reddit_data\n",
        ")\n",
        "\n",
        "# Configure Gemini API (Replace with your own API key)\n",
        "genai.configure(api_key=\"Enter_You_API_Key\")\n",
        "\n",
        "# Define the improved prompt for Gemini Flash\n",
        "prompt = f\"\"\"\n",
        "Analyze the following stock market discussions from Reddit and generate a concise and insightful summary.\n",
        "\n",
        "**Overall Sentiment:**\n",
        "- What is the general mood of investors? (e.g., optimism, fear, uncertainty)\n",
        "- Are there concerns about government policies, economic trends, or specific sectors?\n",
        "\n",
        "**Key Themes:**\n",
        "- Identify the top discussions based on engagement (high upvotes and comments).\n",
        "- Highlight the most talked-about economic trends, policies, market corrections, or investor concerns.\n",
        "\n",
        "**Hot Stocks:**\n",
        "- List stocks that were frequently mentioned.\n",
        "- Explain the context in which these stocks were discussed (bullish/bearish sentiment, earnings, news impact).\n",
        "\n",
        "**Actionable Insights for Investors & Traders:**\n",
        "- What opportunities or risks should traders and investors watch for?\n",
        "- Any recommendations based on sentiment, stock mentions, and market themes?\n",
        "\n",
        "**Reddit Stock Discussion Data:**\n",
        "{all_text}\n",
        "\"\"\"\n",
        "\n",
        "# Call Gemini Flash (Free model)\n",
        "model = genai.GenerativeModel(\"gemini-pro\")\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "# Print the summary\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "Ovwz5BIN34Ce",
        "outputId": "61978918-8d13-44f9-fa65-57032b6eb7a8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Overall Sentiment:**\n",
            "\n",
            "- **Optimism:**\n",
            "    - **Mixed sentiment**, with users expressing concern about market volatility and correction.\n",
            "    - **Some optimism** regarding the long-term growth potential of the Indian economy.\n",
            "- **Fear:**\n",
            "    - **Concerns** regarding government policies, particularly tax burdens on the middle class.\n",
            "    - **Unease** about the impact of global economic trends on the Indian market.\n",
            "- **Uncertainty:**\n",
            "    - **Uncertain** about the future trajectory of the market and the impact of government initiatives.\n",
            "\n",
            "**Key Themes:**\n",
            "\n",
            "- **Tax Revolt Discussion:**\n",
            "    - Extensive discussion on the Nationwide Taxpayers' Revolt, highlighting concerns about excessive taxation.\n",
            "    - Suggestions for alternative revenue generation measures and curbs on government spending.\n",
            "- **Impact of Trump's Speech:**\n",
            "    - Mixed reactions to Trump's speech, with some users expressing concern about tariffs on Indian exports.\n",
            "    - Others highlighting the potential benefits of reduced competition from Chinese goods.\n",
            "- **AI and Technology Trends:**\n",
            "    - Discussions on the potential of AI and technology in India, especially in the context of Deepseek's development.\n",
            "    - Concerns raised about India's lagging research and development investments compared to China.\n",
            "- **Indian Stock Market Performance:**\n",
            "    - Concerns about the Indian rupee's depreciation against the US dollar.\n",
            "    - Discussion on the impact of rising interest rates and inflation on the market.\n",
            "- **Work-Life Balance Debate:**\n",
            "    - Reactions to the 90-hour work week debate, with some users supporting the idea while others emphasizing the importance of work-life balance.\n",
            "\n",
            "**Hot Stocks:**\n",
            "\n",
            "- **Zomato:** Concerns about high stock valuation and potential for losses.\n",
            "- **HDFC Bank:** Discussion about a recent FD fraud case and its potential impact on the bank's reputation.\n",
            "- **Mahindra Group:** Mixed opinions on the company's work culture and labor practices.\n",
            "\n",
            "**Actionable Insights for Investors & Traders:**\n",
            "\n",
            "- **Monitor government policies:** Pay attention to changes in taxation, economic incentives, and regulatory frameworks.\n",
            "- **Diversify portfolios:** Spread investments across different sectors and asset classes to mitigate risks.\n",
            "- **Research potential investments:** Conduct thorough due diligence on companies before making investment decisions.\n",
            "- **Stay informed about economic trends:** Keep track of global and domestic economic indicators that may impact market performance.\n",
            "- **Consider alternative investment strategies:** Explore options such as real estate, bonds, or mutual funds to balance risk and return.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}